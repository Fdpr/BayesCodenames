{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Codemaster testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### loading boards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pickle\n",
    "from agents.rsa.rsagent import fasttext_Codemaster, llama_Codemaster, openai_Codemaster, swow_Codemaster, Codemaster_Wrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"auto_eval/cm/boards.pkl\", \"rb\") as file:\n",
    "    boards = pickle.load(file)\n",
    "with open(\"agents/data/all_codewords.pkl\", \"rb\") as file:\n",
    "    codewords = pickle.load(file)\n",
    "with open(\"agents/data/all_clues.pkl\", \"rb\") as file:\n",
    "    clues = pickle.load(file)[:20000]\n",
    "    \n",
    "labels = [\"fasttext\", \"llama\", \"openai\", \"swow\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for codemaster, label in zip([fasttext_Codemaster, llama_Codemaster, openai_Codemaster, swow_Codemaster], labels):\n",
    "    agent = Codemaster_Wrapper(codemaster(), clues, codewords)\n",
    "    agent.codemaster.weights = np.array([1, -1.7, -1.7, -1.7]).astype(\"float32\")\n",
    "    codemaster_clues = [[*agent.give_clue(board_words, assocs)] for board_words, assocs in boards]\n",
    "    with open(f\"auto_eval/cm/{label}.pkl\", \"wb+\") as file:\n",
    "        pickle.dump(codemaster_clues, file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generating guesses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from agents.rsa.rsagent import fasttext_Guesser, llama_Guesser, openai_Guesser, swow_Guesser, Guesser_Wrapper\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for codemaster in labels:\n",
    "    print(codemaster)\n",
    "    with open(f\"auto_eval/cm/{codemaster}.pkl\", \"rb\") as file:\n",
    "        codemaster_clues = pickle.load(file)\n",
    "    board_guesses = []\n",
    "    for guesser in [fasttext_Guesser, llama_Guesser, openai_Guesser, swow_Guesser]:\n",
    "        agent = Guesser_Wrapper(guesser(), clues, codewords)\n",
    "        agent.guesser.weights = np.array([1, -1.7, -1.7, -1.7]).astype(\"float32\")\n",
    "        guesser_guesses = []\n",
    "        for idx, (clue, count) in enumerate(tqdm(codemaster_clues)):\n",
    "            board_words, assocs = boards[idx]\n",
    "            guesser_guesses.append(agent.guess(assocs, board_words, clue, count))\n",
    "        board_guesses.append(guesser_guesses)\n",
    "    with open(f\"auto_eval/cm/{codemaster}_guesses_variant.pkl\", \"wb+\") as file:\n",
    "        pickle.dump(board_guesses, file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exporting the clues for human evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "board_exp = []\n",
    "for codemaster in labels:\n",
    "    with open(f\"auto_eval/cm/{codemaster}.pkl\", \"rb\") as file:\n",
    "        board_clues = pickle.load(file)\n",
    "        for idx, board in enumerate(boards):\n",
    "            board_exp.append({\n",
    "                \"clue\": board_clues[idx][0],\n",
    "                \"count\": board_clues[idx][1],\n",
    "                \"words\": board[0],\n",
    "                \"assoc\": [0 if x == \"good\" else 1 for x in board[1]]\n",
    "            })\n",
    "with open(\"auto_eval/cm/exp.json\", \"w+\") as file:\n",
    "    json.dump(board_exp, file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "normal_absolute = []\n",
    "normal_ratio = []\n",
    "normal_random_absolute = []\n",
    "normal_random_ratio = []\n",
    "for codemaster in labels:\n",
    "    with open(f\"auto_eval/cm/{codemaster}_guesses.pkl\", \"rb\") as file:\n",
    "        board_guesses = pickle.load(file)\n",
    "    running_correct = 0\n",
    "    running_correct_random = 0\n",
    "    running = 0\n",
    "    running_random = 0\n",
    "    for guesser in board_guesses:\n",
    "        for i, guesses in enumerate(guesser):\n",
    "            if type(guesses) != list:\n",
    "                for guess in guesses:\n",
    "                    if boards[i][1][guess] == \"good\":\n",
    "                        running_correct_random += 1\n",
    "                    running_random += 1\n",
    "            for guess in guesses:\n",
    "                if boards[i][1][guess] == \"good\":\n",
    "                    running_correct += 1\n",
    "                    running_correct_random += 1\n",
    "                running += 1\n",
    "                running_random += 1\n",
    "    normal_absolute.append(running_correct)\n",
    "    normal_ratio.append(running_correct / running)\n",
    "    normal_random_absolute.append(running_correct_random)\n",
    "    normal_random_ratio.append(running_correct_random / running_random)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "variant_absolute = []\n",
    "variant_ratio = []\n",
    "for codemaster in labels:\n",
    "    with open(f\"auto_eval/cm/{codemaster}_guesses_variant.pkl\", \"rb\") as file:\n",
    "        board_guesses = pickle.load(file)\n",
    "    running_correct = 0\n",
    "    running_correct_random = 0\n",
    "    running = 0\n",
    "    running_random = 0\n",
    "    for guesser in board_guesses:\n",
    "        for i, guesses in enumerate(guesser):\n",
    "            for guess in guesses:\n",
    "                if boards[i][1][guess] == \"good\":\n",
    "                    running_correct += 1\n",
    "                running += 1\n",
    "    variant_absolute.append(running_correct)\n",
    "    variant_ratio.append(running_correct / running)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find out which board is from which codemaster because I am stupid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_boards = []\n",
    "for codemaster in labels:\n",
    "    with open(f\"auto_eval/cm/{codemaster}.pkl\", \"rb\") as file:\n",
    "        codemaster_boards = pickle.load(file)\n",
    "    all_boards += [{\"clue\": board[0], \"count\": board[1], \"words\": boards[i][0], \"codemaster\": codemaster} for i, board in enumerate(codemaster_boards)]\n",
    "\n",
    "is_codemaster = lambda board, codemaster: next(b for b in all_boards if b[\"clue\"] == board[\"clue\"] and b[\"count\"] == board[\"count\"] and b[\"words\"] == board[\"words\"])[\"codemaster\"] == codemaster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_correct = lambda board: len([idx for idx in [board[\"words\"].index(guess) for guess in board[\"guesses\"]] if board[\"assoc\"][idx] == 0])\n",
    "\n",
    "human_absolute = []\n",
    "human_ratio = []\n",
    "with open(\"auto_eval/cm/exp_results.json\", \"r\") as file:\n",
    "    human = json.load(file)\n",
    "for codemaster in labels:\n",
    "    codemaster_boards = [board for board in human if is_codemaster(board, codemaster)]\n",
    "    running_correct = 0\n",
    "    running = 0\n",
    "    for board in codemaster_boards:\n",
    "        running_correct += num_correct(board)\n",
    "        running += len(board[\"guesses\"])\n",
    "    human_absolute.append(running_correct)\n",
    "    human_ratio.append(running_correct / running)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(columns=labels,\n",
    "                  data=[normal_absolute, normal_ratio, normal_random_absolute, normal_random_ratio, variant_absolute, variant_ratio, human_absolute, human_ratio],\n",
    "                  index=[\"normal absolute\", \"normal ratio\", \"normal absolute + random\", \"normal ratio + random\", \"variant absolute\", \"variate ratio\", \"human absolute\", \"human ratio\"])\n",
    "\n",
    "def row_max(row):\n",
    "    new_row = [0] * len(row)\n",
    "    max_index = row.idxmax()\n",
    "    new_row[row.index.get_loc(max_index)] = 1\n",
    "    return new_row\n",
    "\n",
    "display(df)\n",
    "df.apply(row_max, axis=1, result_type=\"expand\").rename({i:e for i,e in enumerate(labels)}, axis=\"columns\").sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Guesser Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import pickle\n",
    "from agents.rsa.rsagent import fasttext_Codemaster, llama_Codemaster, openai_Codemaster, swow_Codemaster, Codemaster_Wrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recode = lambda assoc: [\"good\" if x == 0 else \"neutral\" for x in assoc]\n",
    "\n",
    "with open(\"../exp/src/agent_list_1.json\", \"r\") as file:\n",
    "    boards = json.load(file)\n",
    "boards = [[board[\"words\"], recode(board[\"assoc\"])] for board in boards]\n",
    "with open(\"agents/data/all_codewords.pkl\", \"rb\") as file:\n",
    "    codewords = pickle.load(file)\n",
    "with open(\"agents/data/all_clues.pkl\", \"rb\") as file:\n",
    "    clues = pickle.load(file)[:20000]\n",
    "    \n",
    "labels = [\"fasttext\", \"llama\", \"openai\", \"swow\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "codemaster_clues = []\n",
    "\n",
    "for codemaster in [fasttext_Codemaster, llama_Codemaster, openai_Codemaster, swow_Codemaster]:\n",
    "    agent = Codemaster_Wrapper(codemaster(), clues, codewords)\n",
    "    agent.codemaster.weights = np.array([1, -1.7, -1.7, -1.7]).astype(\"float32\")\n",
    "    codemaster_clues += [[*agent.give_clue(board_words, assocs)] + [board_words, assocs] for board_words, assocs in boards]\n",
    "with open(f\"auto_eval/gs/boards_clues.pkl\", \"wb+\") as file:\n",
    "    pickle.dump(codemaster_clues, file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generating guesses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from agents.rsa.rsagent import fasttext_Guesser, llama_Guesser, openai_Guesser, swow_Guesser, Guesser_Wrapper\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"auto_eval/gs/boards_clues.pkl\", \"rb\") as file:\n",
    "    codemaster_clues = pickle.load(file)\n",
    "for guesser, label in zip([fasttext_Guesser, llama_Guesser, openai_Guesser, swow_Guesser], labels):\n",
    "    guesser_guesses = []\n",
    "    agent = Guesser_Wrapper(guesser(), clues, codewords)\n",
    "    agent.guesser.weights = np.array([1, -1.7, -1.7, -1.7]).astype(\"float32\")\n",
    "    for (clue, count, board_words, assocs) in tqdm(codemaster_clues):\n",
    "        guesser_guesses.append(agent.guess(assocs, board_words, clue, count))\n",
    "    with open(f\"auto_eval/gs/{label}.pkl\", \"wb+\") as file:\n",
    "        pickle.dump(guesser_guesses, file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "labels = [\"fasttext\", \"fasttext_variant\", \"llama\", \"llama_variant\", \"openai\", \"openai_variant\", \"swow\", \"swow_variant\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f\"auto_eval/gs/boards_clues.pkl\", \"rb\") as file:\n",
    "    boards = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "normal_absolute = []\n",
    "normal_ratio = []\n",
    "normal_random_absolute = []\n",
    "normal_random_ratio = []\n",
    "for guesser in labels:\n",
    "    with open(f\"auto_eval/gs/{guesser}.pkl\", \"rb\") as file:\n",
    "        board_guesses = pickle.load(file)\n",
    "    running_correct = 0\n",
    "    running_correct_random = 0\n",
    "    running = 0\n",
    "    running_random = 0\n",
    "    for i, guesses in enumerate(board_guesses):\n",
    "        if type(guesses) != list:\n",
    "            for guess in guesses[:1]:\n",
    "                if boards[i][3][guess] == \"good\":\n",
    "                    running_correct_random += 1\n",
    "                running_random += 1\n",
    "                running += 1\n",
    "        else:\n",
    "            for guess in guesses:\n",
    "                if boards[i][3][guess] == \"good\":\n",
    "                    running_correct += 1\n",
    "                    running_correct_random += 1\n",
    "                running += 1\n",
    "                running_random += 1\n",
    "    normal_absolute.append(running_correct)\n",
    "    normal_ratio.append(running_correct / running)\n",
    "    normal_random_absolute.append(running_correct_random)\n",
    "    normal_random_ratio.append(running_correct_random / running_random)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(columns=labels,\n",
    "                  data=[normal_absolute, normal_ratio, normal_random_absolute, normal_random_ratio],\n",
    "                  index=[\"normal absolute\", \"normal ratio\", \"normal absolute + random\", \"normal ratio + random\"])\n",
    "\n",
    "def row_max(row):\n",
    "    new_row = [0] * len(row)\n",
    "    max_index = row.idxmax()\n",
    "    new_row[row.index.get_loc(max_index)] = 1\n",
    "    return new_row\n",
    "\n",
    "display(df)\n",
    "df.apply(row_max, axis=1, result_type=\"expand\").rename({i:e for i,e in enumerate(labels)}, axis=\"columns\").sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Playing Games"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "from tqdm import tqdm\n",
    "from itertools import product\n",
    "from agents.rsa.rsagent import fasttext_Codemaster, llama_Codemaster, openai_Codemaster, swow_Codemaster\n",
    "from agents.rsa.rsagent import fasttext_Guesser, llama_Guesser, openai_Guesser, swow_Guesser\n",
    "from agents.rsa.rsagent import Guesser_Wrapper, Codemaster_Wrapper\n",
    "from agents.others.gpt import Codemaster_Wrapper as GPT\n",
    "from agents.game import Codenames\n",
    "\n",
    "with open(\"agents/data/all_codewords.pkl\", \"rb\") as file:\n",
    "    codewords = pickle.load(file)\n",
    "with open(\"agents/data/all_clues.pkl\", \"rb\") as file:\n",
    "    clues = pickle.load(file)[:20000]\n",
    "\n",
    "boards = []\n",
    "for i in range(1, 6):\n",
    "    with open(f\"agents/data/boards/board_{i}.pkl\", \"rb\") as file:\n",
    "        boards.append(pickle.load(file))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "pairs = list(product(\n",
    "    [(fasttext_Guesser, \"fasttext\"), (llama_Guesser, \"llama\"), (openai_Guesser, \"openai\"), (swow_Guesser, \"swow\")],\n",
    "    [(fasttext_Codemaster, \"fasttext\"), (llama_Codemaster, \"llama\"), (openai_Codemaster, \"openai\"), (swow_Codemaster, \"swow\"), (GPT, \"other-GPT\")]))\n",
    "pairs = [(pair[0][0], pair[0][1], pair[1][0], pair[1][1]) for pair in pairs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "fasttext--other-GPT:   0%|          | 0/5 [00:00<?, ?it/s]c:\\Users\\flohk\\Anaconda3\\envs\\CodenamesRSA\\Lib\\site-packages\\transformers\\models\\auto\\modeling_auto.py:1712: FutureWarning: The class `AutoModelWithLMHead` is deprecated and will be removed in a future version. Please use `AutoModelForCausalLM` for causal language models, `AutoModelForMaskedLM` for masked language models and `AutoModelForSeq2SeqLM` for encoder-decoder models.\n",
      "  warnings.warn(\n",
      "fasttext--other-GPT:  20%|██        | 1/5 [12:34<50:19, 754.90s/it]c:\\Users\\flohk\\Anaconda3\\envs\\CodenamesRSA\\Lib\\site-packages\\transformers\\models\\auto\\modeling_auto.py:1712: FutureWarning: The class `AutoModelWithLMHead` is deprecated and will be removed in a future version. Please use `AutoModelForCausalLM` for causal language models, `AutoModelForMaskedLM` for masked language models and `AutoModelForSeq2SeqLM` for encoder-decoder models.\n",
      "  warnings.warn(\n",
      "fasttext--other-GPT:  40%|████      | 2/5 [21:01<30:26, 608.68s/it]c:\\Users\\flohk\\Anaconda3\\envs\\CodenamesRSA\\Lib\\site-packages\\transformers\\models\\auto\\modeling_auto.py:1712: FutureWarning: The class `AutoModelWithLMHead` is deprecated and will be removed in a future version. Please use `AutoModelForCausalLM` for causal language models, `AutoModelForMaskedLM` for masked language models and `AutoModelForSeq2SeqLM` for encoder-decoder models.\n",
      "  warnings.warn(\n",
      "fasttext--other-GPT:  60%|██████    | 3/5 [32:59<21:57, 658.83s/it]c:\\Users\\flohk\\Anaconda3\\envs\\CodenamesRSA\\Lib\\site-packages\\transformers\\models\\auto\\modeling_auto.py:1712: FutureWarning: The class `AutoModelWithLMHead` is deprecated and will be removed in a future version. Please use `AutoModelForCausalLM` for causal language models, `AutoModelForMaskedLM` for masked language models and `AutoModelForSeq2SeqLM` for encoder-decoder models.\n",
      "  warnings.warn(\n",
      "c:\\Users\\flohk\\OneDrive\\Uni Master\\Masterarbeit\\BayesCodenames\\agents\\game.py:74: RuntimeWarning: invalid value encountered in divide\n",
      "  samples = .5 + (samples / samples.max())\n",
      "fasttext--other-GPT:  80%|████████  | 4/5 [43:30<10:47, 647.86s/it]c:\\Users\\flohk\\Anaconda3\\envs\\CodenamesRSA\\Lib\\site-packages\\transformers\\models\\auto\\modeling_auto.py:1712: FutureWarning: The class `AutoModelWithLMHead` is deprecated and will be removed in a future version. Please use `AutoModelForCausalLM` for causal language models, `AutoModelForMaskedLM` for masked language models and `AutoModelForSeq2SeqLM` for encoder-decoder models.\n",
      "  warnings.warn(\n",
      "fasttext--other-GPT: 100%|██████████| 5/5 [45:52<00:00, 550.56s/it]\n",
      "llama--other-GPT:   0%|          | 0/5 [00:00<?, ?it/s]c:\\Users\\flohk\\Anaconda3\\envs\\CodenamesRSA\\Lib\\site-packages\\transformers\\models\\auto\\modeling_auto.py:1712: FutureWarning: The class `AutoModelWithLMHead` is deprecated and will be removed in a future version. Please use `AutoModelForCausalLM` for causal language models, `AutoModelForMaskedLM` for masked language models and `AutoModelForSeq2SeqLM` for encoder-decoder models.\n",
      "  warnings.warn(\n",
      "llama--other-GPT:  20%|██        | 1/5 [11:34<46:19, 694.78s/it]c:\\Users\\flohk\\Anaconda3\\envs\\CodenamesRSA\\Lib\\site-packages\\transformers\\models\\auto\\modeling_auto.py:1712: FutureWarning: The class `AutoModelWithLMHead` is deprecated and will be removed in a future version. Please use `AutoModelForCausalLM` for causal language models, `AutoModelForMaskedLM` for masked language models and `AutoModelForSeq2SeqLM` for encoder-decoder models.\n",
      "  warnings.warn(\n",
      "llama--other-GPT:  40%|████      | 2/5 [23:42<35:42, 714.33s/it]c:\\Users\\flohk\\Anaconda3\\envs\\CodenamesRSA\\Lib\\site-packages\\transformers\\models\\auto\\modeling_auto.py:1712: FutureWarning: The class `AutoModelWithLMHead` is deprecated and will be removed in a future version. Please use `AutoModelForCausalLM` for causal language models, `AutoModelForMaskedLM` for masked language models and `AutoModelForSeq2SeqLM` for encoder-decoder models.\n",
      "  warnings.warn(\n",
      "llama--other-GPT:  60%|██████    | 3/5 [33:47<22:08, 664.46s/it]c:\\Users\\flohk\\Anaconda3\\envs\\CodenamesRSA\\Lib\\site-packages\\transformers\\models\\auto\\modeling_auto.py:1712: FutureWarning: The class `AutoModelWithLMHead` is deprecated and will be removed in a future version. Please use `AutoModelForCausalLM` for causal language models, `AutoModelForMaskedLM` for masked language models and `AutoModelForSeq2SeqLM` for encoder-decoder models.\n",
      "  warnings.warn(\n",
      "llama--other-GPT:  80%|████████  | 4/5 [36:10<07:38, 458.39s/it]c:\\Users\\flohk\\Anaconda3\\envs\\CodenamesRSA\\Lib\\site-packages\\transformers\\models\\auto\\modeling_auto.py:1712: FutureWarning: The class `AutoModelWithLMHead` is deprecated and will be removed in a future version. Please use `AutoModelForCausalLM` for causal language models, `AutoModelForMaskedLM` for masked language models and `AutoModelForSeq2SeqLM` for encoder-decoder models.\n",
      "  warnings.warn(\n",
      "c:\\Users\\flohk\\OneDrive\\Uni Master\\Masterarbeit\\BayesCodenames\\agents\\game.py:74: RuntimeWarning: invalid value encountered in divide\n",
      "  samples = .5 + (samples / samples.max())\n",
      "llama--other-GPT: 100%|██████████| 5/5 [48:17<00:00, 579.47s/it]\n",
      "openai--other-GPT:   0%|          | 0/5 [00:00<?, ?it/s]c:\\Users\\flohk\\Anaconda3\\envs\\CodenamesRSA\\Lib\\site-packages\\transformers\\models\\auto\\modeling_auto.py:1712: FutureWarning: The class `AutoModelWithLMHead` is deprecated and will be removed in a future version. Please use `AutoModelForCausalLM` for causal language models, `AutoModelForMaskedLM` for masked language models and `AutoModelForSeq2SeqLM` for encoder-decoder models.\n",
      "  warnings.warn(\n",
      "openai--other-GPT:  20%|██        | 1/5 [11:44<46:57, 704.37s/it]c:\\Users\\flohk\\Anaconda3\\envs\\CodenamesRSA\\Lib\\site-packages\\transformers\\models\\auto\\modeling_auto.py:1712: FutureWarning: The class `AutoModelWithLMHead` is deprecated and will be removed in a future version. Please use `AutoModelForCausalLM` for causal language models, `AutoModelForMaskedLM` for masked language models and `AutoModelForSeq2SeqLM` for encoder-decoder models.\n",
      "  warnings.warn(\n",
      "openai--other-GPT:  40%|████      | 2/5 [23:39<35:32, 710.74s/it]c:\\Users\\flohk\\Anaconda3\\envs\\CodenamesRSA\\Lib\\site-packages\\transformers\\models\\auto\\modeling_auto.py:1712: FutureWarning: The class `AutoModelWithLMHead` is deprecated and will be removed in a future version. Please use `AutoModelForCausalLM` for causal language models, `AutoModelForMaskedLM` for masked language models and `AutoModelForSeq2SeqLM` for encoder-decoder models.\n",
      "  warnings.warn(\n",
      "openai--other-GPT:  60%|██████    | 3/5 [32:09<20:37, 618.98s/it]c:\\Users\\flohk\\Anaconda3\\envs\\CodenamesRSA\\Lib\\site-packages\\transformers\\models\\auto\\modeling_auto.py:1712: FutureWarning: The class `AutoModelWithLMHead` is deprecated and will be removed in a future version. Please use `AutoModelForCausalLM` for causal language models, `AutoModelForMaskedLM` for masked language models and `AutoModelForSeq2SeqLM` for encoder-decoder models.\n",
      "  warnings.warn(\n",
      "openai--other-GPT:  80%|████████  | 4/5 [40:36<09:35, 575.02s/it]c:\\Users\\flohk\\Anaconda3\\envs\\CodenamesRSA\\Lib\\site-packages\\transformers\\models\\auto\\modeling_auto.py:1712: FutureWarning: The class `AutoModelWithLMHead` is deprecated and will be removed in a future version. Please use `AutoModelForCausalLM` for causal language models, `AutoModelForMaskedLM` for masked language models and `AutoModelForSeq2SeqLM` for encoder-decoder models.\n",
      "  warnings.warn(\n",
      "openai--other-GPT: 100%|██████████| 5/5 [50:50<00:00, 610.02s/it]\n",
      "swow--other-GPT:   0%|          | 0/5 [00:00<?, ?it/s]c:\\Users\\flohk\\Anaconda3\\envs\\CodenamesRSA\\Lib\\site-packages\\transformers\\models\\auto\\modeling_auto.py:1712: FutureWarning: The class `AutoModelWithLMHead` is deprecated and will be removed in a future version. Please use `AutoModelForCausalLM` for causal language models, `AutoModelForMaskedLM` for masked language models and `AutoModelForSeq2SeqLM` for encoder-decoder models.\n",
      "  warnings.warn(\n",
      "swow--other-GPT:  20%|██        | 1/5 [02:23<09:33, 143.48s/it]c:\\Users\\flohk\\Anaconda3\\envs\\CodenamesRSA\\Lib\\site-packages\\transformers\\models\\auto\\modeling_auto.py:1712: FutureWarning: The class `AutoModelWithLMHead` is deprecated and will be removed in a future version. Please use `AutoModelForCausalLM` for causal language models, `AutoModelForMaskedLM` for masked language models and `AutoModelForSeq2SeqLM` for encoder-decoder models.\n",
      "  warnings.warn(\n",
      "swow--other-GPT:  40%|████      | 2/5 [12:38<21:02, 420.95s/it]c:\\Users\\flohk\\Anaconda3\\envs\\CodenamesRSA\\Lib\\site-packages\\transformers\\models\\auto\\modeling_auto.py:1712: FutureWarning: The class `AutoModelWithLMHead` is deprecated and will be removed in a future version. Please use `AutoModelForCausalLM` for causal language models, `AutoModelForMaskedLM` for masked language models and `AutoModelForSeq2SeqLM` for encoder-decoder models.\n",
      "  warnings.warn(\n",
      "swow--other-GPT:  60%|██████    | 3/5 [26:12<20:01, 600.56s/it]c:\\Users\\flohk\\Anaconda3\\envs\\CodenamesRSA\\Lib\\site-packages\\transformers\\models\\auto\\modeling_auto.py:1712: FutureWarning: The class `AutoModelWithLMHead` is deprecated and will be removed in a future version. Please use `AutoModelForCausalLM` for causal language models, `AutoModelForMaskedLM` for masked language models and `AutoModelForSeq2SeqLM` for encoder-decoder models.\n",
      "  warnings.warn(\n",
      "swow--other-GPT:  80%|████████  | 4/5 [45:40<13:44, 824.58s/it]c:\\Users\\flohk\\Anaconda3\\envs\\CodenamesRSA\\Lib\\site-packages\\transformers\\models\\auto\\modeling_auto.py:1712: FutureWarning: The class `AutoModelWithLMHead` is deprecated and will be removed in a future version. Please use `AutoModelForCausalLM` for causal language models, `AutoModelForMaskedLM` for masked language models and `AutoModelForSeq2SeqLM` for encoder-decoder models.\n",
      "  warnings.warn(\n",
      "swow--other-GPT: 100%|██████████| 5/5 [59:47<00:00, 717.46s/it]\n"
     ]
    }
   ],
   "source": [
    "for (guesser, guesser_name), (codemaster, codemaster_name) in product(\n",
    "    [(fasttext_Guesser, \"fasttext\"), (llama_Guesser, \"llama\"), (openai_Guesser, \"openai\"), (swow_Guesser, \"swow\")],\n",
    "    [(fasttext_Codemaster, \"fasttext\"), (llama_Codemaster, \"llama\"), (openai_Codemaster, \"openai\"), (swow_Codemaster, \"swow\"), (GPT, \"other-GPT\")]):\n",
    "    states = []\n",
    "    roundss = []\n",
    "    scoress = []\n",
    "    for board in tqdm(boards, desc=f\"{guesser_name}--{codemaster_name}\"):\n",
    "        guesser_agent = Guesser_Wrapper(guesser(), clues, codewords, mcmc_burn_in=1_000, mcmc_iter=300_000, variant=True)\n",
    "        codemaster_agent = Codemaster_Wrapper(codemaster(), clues, codewords) if codemaster_name != \"other-GPT\" else GPT(clues)\n",
    "        state, rounds, scores = Codenames(board[0], board[1], guesser_agent, codemaster_agent).play_game()\n",
    "        states.append(state)\n",
    "        roundss.append(rounds)\n",
    "        scoress.append(scores)\n",
    "    with open(f\"auto_eval/games/{guesser_name}_variant--{codemaster_name}.pkl\", \"wb+\") as file:\n",
    "        pickle.dump([guesser_name, codemaster_name, states, roundss, scoress], file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "fasttext--openai: 100%|██████████| 5/5 [4:08:09<00:00, 2977.99s/it]  \n",
      "fasttext--swow: 100%|██████████| 5/5 [4:23:49<00:00, 3165.81s/it]\n",
      "fasttext--other-GPT:   0%|          | 0/5 [00:00<?, ?it/s]c:\\Users\\flohk\\Anaconda3\\envs\\CodenamesRSA\\Lib\\site-packages\\transformers\\models\\auto\\modeling_auto.py:1712: FutureWarning: The class `AutoModelWithLMHead` is deprecated and will be removed in a future version. Please use `AutoModelForCausalLM` for causal language models, `AutoModelForMaskedLM` for masked language models and `AutoModelForSeq2SeqLM` for encoder-decoder models.\n",
      "  warnings.warn(\n",
      "fasttext--other-GPT:  20%|██        | 1/5 [15:56<1:03:45, 956.46s/it]c:\\Users\\flohk\\Anaconda3\\envs\\CodenamesRSA\\Lib\\site-packages\\transformers\\models\\auto\\modeling_auto.py:1712: FutureWarning: The class `AutoModelWithLMHead` is deprecated and will be removed in a future version. Please use `AutoModelForCausalLM` for causal language models, `AutoModelForMaskedLM` for masked language models and `AutoModelForSeq2SeqLM` for encoder-decoder models.\n",
      "  warnings.warn(\n",
      "fasttext--other-GPT:  40%|████      | 2/5 [34:32<52:31, 1050.37s/it] c:\\Users\\flohk\\Anaconda3\\envs\\CodenamesRSA\\Lib\\site-packages\\transformers\\models\\auto\\modeling_auto.py:1712: FutureWarning: The class `AutoModelWithLMHead` is deprecated and will be removed in a future version. Please use `AutoModelForCausalLM` for causal language models, `AutoModelForMaskedLM` for masked language models and `AutoModelForSeq2SeqLM` for encoder-decoder models.\n",
      "  warnings.warn(\n",
      "fasttext--other-GPT:  60%|██████    | 3/5 [44:55<28:30, 855.12s/it] c:\\Users\\flohk\\Anaconda3\\envs\\CodenamesRSA\\Lib\\site-packages\\transformers\\models\\auto\\modeling_auto.py:1712: FutureWarning: The class `AutoModelWithLMHead` is deprecated and will be removed in a future version. Please use `AutoModelForCausalLM` for causal language models, `AutoModelForMaskedLM` for masked language models and `AutoModelForSeq2SeqLM` for encoder-decoder models.\n",
      "  warnings.warn(\n",
      "fasttext--other-GPT:  80%|████████  | 4/5 [57:37<13:38, 818.32s/it]c:\\Users\\flohk\\Anaconda3\\envs\\CodenamesRSA\\Lib\\site-packages\\transformers\\models\\auto\\modeling_auto.py:1712: FutureWarning: The class `AutoModelWithLMHead` is deprecated and will be removed in a future version. Please use `AutoModelForCausalLM` for causal language models, `AutoModelForMaskedLM` for masked language models and `AutoModelForSeq2SeqLM` for encoder-decoder models.\n",
      "  warnings.warn(\n",
      "fasttext--other-GPT: 100%|██████████| 5/5 [1:04:11<00:00, 770.25s/it]\n"
     ]
    }
   ],
   "source": [
    "for (guesser, guesser_name), (codemaster, codemaster_name) in product(\n",
    "    [(fasttext_Guesser, \"fasttext\"), (llama_Guesser, \"llama\"), (openai_Guesser, \"openai\"), (swow_Guesser, \"swow\")],\n",
    "    [(fasttext_Codemaster, \"fasttext\"), (llama_Codemaster, \"llama\"), (openai_Codemaster, \"openai\"), (swow_Codemaster, \"swow\"), (GPT, \"other-GPT\")]):\n",
    "    states = []\n",
    "    roundss = []\n",
    "    scoress = []\n",
    "    for board in tqdm(boards, desc=f\"{guesser_name}--{codemaster_name}\"):\n",
    "        guesser_agent = Guesser_Wrapper(guesser(), clues, codewords, mcmc_burn_in=1_000, mcmc_iter=200_000, variant=False)\n",
    "        codemaster_agent = Codemaster_Wrapper(codemaster(), clues, codewords) if codemaster_name != \"other-GPT\" else GPT(clues)\n",
    "        state, rounds, scores = Codenames(board[0], board[1], guesser_agent, codemaster_agent).play_game()\n",
    "        states.append(state)\n",
    "        roundss.append(rounds)\n",
    "        scoress.append(scores)\n",
    "    with open(f\"auto_eval/games/{guesser_name}--{codemaster_name}.pkl\", \"wb+\") as file:\n",
    "        pickle.dump([guesser_name, codemaster_name, states, roundss, scoress], file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "fasttext--fasttext: 100%|██████████| 5/5 [00:10<00:00,  2.07s/it]\n",
      "fasttext--llama: 100%|██████████| 5/5 [00:04<00:00,  1.24it/s]\n",
      "fasttext--openai: 100%|██████████| 5/5 [00:03<00:00,  1.30it/s]\n",
      "fasttext--swow: 100%|██████████| 5/5 [00:04<00:00,  1.20it/s]\n",
      "fasttext--other-GPT:   0%|          | 0/5 [00:00<?, ?it/s]c:\\Users\\flohk\\Anaconda3\\envs\\CodenamesRSA\\Lib\\site-packages\\transformers\\models\\auto\\modeling_auto.py:1712: FutureWarning: The class `AutoModelWithLMHead` is deprecated and will be removed in a future version. Please use `AutoModelForCausalLM` for causal language models, `AutoModelForMaskedLM` for masked language models and `AutoModelForSeq2SeqLM` for encoder-decoder models.\n",
      "  warnings.warn(\n",
      "fasttext--other-GPT:  20%|██        | 1/5 [00:05<00:21,  5.33s/it]c:\\Users\\flohk\\Anaconda3\\envs\\CodenamesRSA\\Lib\\site-packages\\transformers\\models\\auto\\modeling_auto.py:1712: FutureWarning: The class `AutoModelWithLMHead` is deprecated and will be removed in a future version. Please use `AutoModelForCausalLM` for causal language models, `AutoModelForMaskedLM` for masked language models and `AutoModelForSeq2SeqLM` for encoder-decoder models.\n",
      "  warnings.warn(\n",
      "fasttext--other-GPT:  40%|████      | 2/5 [00:09<00:13,  4.66s/it]c:\\Users\\flohk\\Anaconda3\\envs\\CodenamesRSA\\Lib\\site-packages\\transformers\\models\\auto\\modeling_auto.py:1712: FutureWarning: The class `AutoModelWithLMHead` is deprecated and will be removed in a future version. Please use `AutoModelForCausalLM` for causal language models, `AutoModelForMaskedLM` for masked language models and `AutoModelForSeq2SeqLM` for encoder-decoder models.\n",
      "  warnings.warn(\n",
      "fasttext--other-GPT:  60%|██████    | 3/5 [00:13<00:08,  4.18s/it]c:\\Users\\flohk\\Anaconda3\\envs\\CodenamesRSA\\Lib\\site-packages\\transformers\\models\\auto\\modeling_auto.py:1712: FutureWarning: The class `AutoModelWithLMHead` is deprecated and will be removed in a future version. Please use `AutoModelForCausalLM` for causal language models, `AutoModelForMaskedLM` for masked language models and `AutoModelForSeq2SeqLM` for encoder-decoder models.\n",
      "  warnings.warn(\n",
      "fasttext--other-GPT:  80%|████████  | 4/5 [00:17<00:04,  4.16s/it]c:\\Users\\flohk\\Anaconda3\\envs\\CodenamesRSA\\Lib\\site-packages\\transformers\\models\\auto\\modeling_auto.py:1712: FutureWarning: The class `AutoModelWithLMHead` is deprecated and will be removed in a future version. Please use `AutoModelForCausalLM` for causal language models, `AutoModelForMaskedLM` for masked language models and `AutoModelForSeq2SeqLM` for encoder-decoder models.\n",
      "  warnings.warn(\n",
      "fasttext--other-GPT: 100%|██████████| 5/5 [00:18<00:00,  3.72s/it]\n",
      "llama--fasttext: 100%|██████████| 5/5 [00:04<00:00,  1.21it/s]\n",
      "llama--llama: 100%|██████████| 5/5 [00:03<00:00,  1.39it/s]\n",
      "llama--openai: 100%|██████████| 5/5 [00:03<00:00,  1.46it/s]\n",
      "llama--swow: 100%|██████████| 5/5 [00:04<00:00,  1.09it/s]\n",
      "llama--other-GPT:   0%|          | 0/5 [00:00<?, ?it/s]c:\\Users\\flohk\\Anaconda3\\envs\\CodenamesRSA\\Lib\\site-packages\\transformers\\models\\auto\\modeling_auto.py:1712: FutureWarning: The class `AutoModelWithLMHead` is deprecated and will be removed in a future version. Please use `AutoModelForCausalLM` for causal language models, `AutoModelForMaskedLM` for masked language models and `AutoModelForSeq2SeqLM` for encoder-decoder models.\n",
      "  warnings.warn(\n",
      "llama--other-GPT:  20%|██        | 1/5 [00:06<00:25,  6.38s/it]c:\\Users\\flohk\\Anaconda3\\envs\\CodenamesRSA\\Lib\\site-packages\\transformers\\models\\auto\\modeling_auto.py:1712: FutureWarning: The class `AutoModelWithLMHead` is deprecated and will be removed in a future version. Please use `AutoModelForCausalLM` for causal language models, `AutoModelForMaskedLM` for masked language models and `AutoModelForSeq2SeqLM` for encoder-decoder models.\n",
      "  warnings.warn(\n",
      "llama--other-GPT:  40%|████      | 2/5 [00:10<00:14,  4.78s/it]c:\\Users\\flohk\\Anaconda3\\envs\\CodenamesRSA\\Lib\\site-packages\\transformers\\models\\auto\\modeling_auto.py:1712: FutureWarning: The class `AutoModelWithLMHead` is deprecated and will be removed in a future version. Please use `AutoModelForCausalLM` for causal language models, `AutoModelForMaskedLM` for masked language models and `AutoModelForSeq2SeqLM` for encoder-decoder models.\n",
      "  warnings.warn(\n",
      "llama--other-GPT:  60%|██████    | 3/5 [00:13<00:08,  4.38s/it]c:\\Users\\flohk\\Anaconda3\\envs\\CodenamesRSA\\Lib\\site-packages\\transformers\\models\\auto\\modeling_auto.py:1712: FutureWarning: The class `AutoModelWithLMHead` is deprecated and will be removed in a future version. Please use `AutoModelForCausalLM` for causal language models, `AutoModelForMaskedLM` for masked language models and `AutoModelForSeq2SeqLM` for encoder-decoder models.\n",
      "  warnings.warn(\n",
      "llama--other-GPT:  80%|████████  | 4/5 [00:15<00:03,  3.28s/it]c:\\Users\\flohk\\Anaconda3\\envs\\CodenamesRSA\\Lib\\site-packages\\transformers\\models\\auto\\modeling_auto.py:1712: FutureWarning: The class `AutoModelWithLMHead` is deprecated and will be removed in a future version. Please use `AutoModelForCausalLM` for causal language models, `AutoModelForMaskedLM` for masked language models and `AutoModelForSeq2SeqLM` for encoder-decoder models.\n",
      "  warnings.warn(\n",
      "llama--other-GPT: 100%|██████████| 5/5 [00:19<00:00,  3.98s/it]\n",
      "openai--fasttext: 100%|██████████| 5/5 [00:04<00:00,  1.24it/s]\n",
      "openai--llama: 100%|██████████| 5/5 [00:04<00:00,  1.22it/s]\n",
      "openai--openai: 100%|██████████| 5/5 [00:03<00:00,  1.41it/s]\n",
      "openai--swow: 100%|██████████| 5/5 [00:04<00:00,  1.24it/s]\n",
      "openai--other-GPT:   0%|          | 0/5 [00:00<?, ?it/s]c:\\Users\\flohk\\Anaconda3\\envs\\CodenamesRSA\\Lib\\site-packages\\transformers\\models\\auto\\modeling_auto.py:1712: FutureWarning: The class `AutoModelWithLMHead` is deprecated and will be removed in a future version. Please use `AutoModelForCausalLM` for causal language models, `AutoModelForMaskedLM` for masked language models and `AutoModelForSeq2SeqLM` for encoder-decoder models.\n",
      "  warnings.warn(\n",
      "openai--other-GPT:  20%|██        | 1/5 [00:03<00:13,  3.41s/it]c:\\Users\\flohk\\Anaconda3\\envs\\CodenamesRSA\\Lib\\site-packages\\transformers\\models\\auto\\modeling_auto.py:1712: FutureWarning: The class `AutoModelWithLMHead` is deprecated and will be removed in a future version. Please use `AutoModelForCausalLM` for causal language models, `AutoModelForMaskedLM` for masked language models and `AutoModelForSeq2SeqLM` for encoder-decoder models.\n",
      "  warnings.warn(\n",
      "openai--other-GPT:  40%|████      | 2/5 [00:07<00:11,  3.80s/it]c:\\Users\\flohk\\Anaconda3\\envs\\CodenamesRSA\\Lib\\site-packages\\transformers\\models\\auto\\modeling_auto.py:1712: FutureWarning: The class `AutoModelWithLMHead` is deprecated and will be removed in a future version. Please use `AutoModelForCausalLM` for causal language models, `AutoModelForMaskedLM` for masked language models and `AutoModelForSeq2SeqLM` for encoder-decoder models.\n",
      "  warnings.warn(\n",
      "openai--other-GPT:  60%|██████    | 3/5 [00:10<00:06,  3.45s/it]c:\\Users\\flohk\\Anaconda3\\envs\\CodenamesRSA\\Lib\\site-packages\\transformers\\models\\auto\\modeling_auto.py:1712: FutureWarning: The class `AutoModelWithLMHead` is deprecated and will be removed in a future version. Please use `AutoModelForCausalLM` for causal language models, `AutoModelForMaskedLM` for masked language models and `AutoModelForSeq2SeqLM` for encoder-decoder models.\n",
      "  warnings.warn(\n",
      "openai--other-GPT:  80%|████████  | 4/5 [00:13<00:03,  3.21s/it]c:\\Users\\flohk\\Anaconda3\\envs\\CodenamesRSA\\Lib\\site-packages\\transformers\\models\\auto\\modeling_auto.py:1712: FutureWarning: The class `AutoModelWithLMHead` is deprecated and will be removed in a future version. Please use `AutoModelForCausalLM` for causal language models, `AutoModelForMaskedLM` for masked language models and `AutoModelForSeq2SeqLM` for encoder-decoder models.\n",
      "  warnings.warn(\n",
      "openai--other-GPT: 100%|██████████| 5/5 [00:16<00:00,  3.25s/it]\n",
      "swow--fasttext: 100%|██████████| 5/5 [00:04<00:00,  1.06it/s]\n",
      "swow--llama: 100%|██████████| 5/5 [00:04<00:00,  1.25it/s]\n",
      "swow--openai: 100%|██████████| 5/5 [00:03<00:00,  1.29it/s]\n",
      "swow--swow: 100%|██████████| 5/5 [00:04<00:00,  1.18it/s]\n",
      "swow--other-GPT:   0%|          | 0/5 [00:00<?, ?it/s]c:\\Users\\flohk\\Anaconda3\\envs\\CodenamesRSA\\Lib\\site-packages\\transformers\\models\\auto\\modeling_auto.py:1712: FutureWarning: The class `AutoModelWithLMHead` is deprecated and will be removed in a future version. Please use `AutoModelForCausalLM` for causal language models, `AutoModelForMaskedLM` for masked language models and `AutoModelForSeq2SeqLM` for encoder-decoder models.\n",
      "  warnings.warn(\n",
      "swow--other-GPT:  20%|██        | 1/5 [00:04<00:17,  4.40s/it]c:\\Users\\flohk\\Anaconda3\\envs\\CodenamesRSA\\Lib\\site-packages\\transformers\\models\\auto\\modeling_auto.py:1712: FutureWarning: The class `AutoModelWithLMHead` is deprecated and will be removed in a future version. Please use `AutoModelForCausalLM` for causal language models, `AutoModelForMaskedLM` for masked language models and `AutoModelForSeq2SeqLM` for encoder-decoder models.\n",
      "  warnings.warn(\n",
      "swow--other-GPT:  40%|████      | 2/5 [00:08<00:12,  4.12s/it]c:\\Users\\flohk\\Anaconda3\\envs\\CodenamesRSA\\Lib\\site-packages\\transformers\\models\\auto\\modeling_auto.py:1712: FutureWarning: The class `AutoModelWithLMHead` is deprecated and will be removed in a future version. Please use `AutoModelForCausalLM` for causal language models, `AutoModelForMaskedLM` for masked language models and `AutoModelForSeq2SeqLM` for encoder-decoder models.\n",
      "  warnings.warn(\n",
      "swow--other-GPT:  60%|██████    | 3/5 [00:11<00:07,  3.74s/it]c:\\Users\\flohk\\Anaconda3\\envs\\CodenamesRSA\\Lib\\site-packages\\transformers\\models\\auto\\modeling_auto.py:1712: FutureWarning: The class `AutoModelWithLMHead` is deprecated and will be removed in a future version. Please use `AutoModelForCausalLM` for causal language models, `AutoModelForMaskedLM` for masked language models and `AutoModelForSeq2SeqLM` for encoder-decoder models.\n",
      "  warnings.warn(\n",
      "swow--other-GPT:  80%|████████  | 4/5 [00:17<00:04,  4.41s/it]c:\\Users\\flohk\\Anaconda3\\envs\\CodenamesRSA\\Lib\\site-packages\\transformers\\models\\auto\\modeling_auto.py:1712: FutureWarning: The class `AutoModelWithLMHead` is deprecated and will be removed in a future version. Please use `AutoModelForCausalLM` for causal language models, `AutoModelForMaskedLM` for masked language models and `AutoModelForSeq2SeqLM` for encoder-decoder models.\n",
      "  warnings.warn(\n",
      "swow--other-GPT: 100%|██████████| 5/5 [00:19<00:00,  3.83s/it]\n"
     ]
    }
   ],
   "source": [
    "for (guesser, guesser_name), (codemaster, codemaster_name) in product(\n",
    "    [(fasttext_Guesser, \"fasttext\"), (llama_Guesser, \"llama\"), (openai_Guesser, \"openai\"), (swow_Guesser, \"swow\")],\n",
    "    [(fasttext_Codemaster, \"fasttext\"), (llama_Codemaster, \"llama\"), (openai_Codemaster, \"openai\"), (swow_Codemaster, \"swow\"), (GPT, \"other-GPT\")]):\n",
    "    states = []\n",
    "    roundss = []\n",
    "    scoress = []\n",
    "    for board in tqdm(boards, desc=f\"{guesser_name}--{codemaster_name}\"):\n",
    "        guesser_agent = Guesser_Wrapper(guesser(), clues, codewords, mcmc_burn_in=1_000, mcmc_iter=200_000, variant=False)\n",
    "        codemaster_agent = Codemaster_Wrapper(codemaster(), clues, codewords) if codemaster_name != \"other-GPT\" else GPT(clues)\n",
    "        state, rounds, scores = Codenames(board[0], board[1], guesser_agent, codemaster_agent, simple=True).play_game()\n",
    "        states.append(state)\n",
    "        roundss.append(rounds)\n",
    "        scoress.append(scores)\n",
    "    with open(f\"auto_eval/games/{guesser_name}_simple--{codemaster_name}.pkl\", \"wb+\") as file:\n",
    "        pickle.dump([guesser_name, codemaster_name, states, roundss, scoress], file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "guessers = list(map(lambda arr: arr[0] + arr[1], product([\"fasttext\", \"llama\", \"openai\", \"swow\"], [\"\", \"_variant\", \"_simple\"])))\n",
    "codemasters = [\"fasttext\", \"llama\", \"openai\", \"other-GPT\", \"swow\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['fasttext',\n",
       " 'swow',\n",
       " [3, 3, 2, 2, 2],\n",
       " [1, 4, 5, 7, 6],\n",
       " [[-7.5],\n",
       "  [1, -2, -2, -7.5],\n",
       "  [-1, -2, -2, -1, 1],\n",
       "  [-1, 1, -1, -1, 1, 2, 1],\n",
       "  [-1, -1, -2, 1, -2, -1]]]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open(\"auto_eval/games/fasttext--swow.pkl\", \"rb\") as file:\n",
    "    f = pickle.load(file)\n",
    "f"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "CodenamesRSA",
   "language": "python",
   "name": "codenamesrsa"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
